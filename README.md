# Servicio 1: Model API (LLM)

## üìã Descripci√≥n
API Flask dedicada exclusivamente a inferencia del modelo de lenguaje (LLM).

## üîß Caracter√≠sticas
- Framework: Flask
- Modelo: llama-cpp-python con GGUF
- Puerto: 8001
- Tipo: **Serverless OK** ‚úÖ

## üì¶ Dependencias
```bash
pip install -r requirements.txt
```

## üöÄ Ejecuci√≥n Local
```bash
# Descargar modelo GGUF (si no lo tienes)
mkdir -p models
wget -O models/qwen2.5-3b-instruct-q4_k_m.gguf \
  https://huggingface.co/Qwen/Qwen2.5-3B-Instruct-GGUF/resolve/main/qwen2.5-3b-instruct-q4_k_m.gguf

# Configurar variables de entorno
export LOCAL_MODEL_PATH=./models/qwen2.5-3b-instruct-q4_k_m.gguf
export N_CTX=2048
export N_THREADS=1

# Ejecutar
python model_api.py
```

El servicio estar√° disponible en `http://localhost:8001`

## üåê Variables de Entorno

| Variable | Descripci√≥n | Valor por Defecto |
|----------|-------------|-------------------|
| `LOCAL_MODEL_PATH` | Ruta al archivo GGUF | `./models/qwen2.5-3b-instruct-q4_k_m.gguf` |
| `N_CTX` | Tama√±o del contexto | `2048` |
| `N_THREADS` | N√∫mero de threads CPU | `1` |
| `PORT` | Puerto del servicio | `8001` |

## üì° Endpoints

### GET /health
Health check del servicio.

**Response:**
```json
{
  "status": "ok"
}
```

### POST /chat
Inferencia del modelo LLM.

**Request:**
```json
{
  "system": "Eres un asistente agr√≥nomo...",
  "context": {
    "mensaje": "¬øCu√°ndo debo regar?",
    "productor": {...}
  },
  "max_tokens": 300
}
```

**Response:**
```json
{
  "content": "{\"role\": \"consulta\", \"respuesta_chat\": \"...\", ...}"
}
```

## üö¢ Despliegue en Leapcell

### Paso 1: Crear Proyecto
1. Ir a [Leapcell](https://leapcell.io)
2. New Project ‚Üí Connect GitHub
3. Seleccionar este repositorio
4. Root Directory: `service-1-model`

### Paso 2: Configuraci√≥n
```
Build Command: pip install -r requirements.txt
Start Command: python model_api.py
Port: 8001
```

### Paso 3: Variables de Entorno
Configurar en el panel de Leapcell:
```
LOCAL_MODEL_PATH=/app/models/qwen2.5-3b-instruct-q4_k_m.gguf
N_CTX=2048
N_THREADS=1
```

### Paso 4: Subir Modelo GGUF
‚ö†Ô∏è **IMPORTANTE**: El modelo pesa 3-4 GB

Opciones:
1. **Volumen persistente** (recomendado)
   - Crear volumen en Leapcell
   - Montar en `/app/models`
   - Subir archivo GGUF

2. **Descargar en build**
   - Crear script de inicio que descargue el modelo
   - Cachear en volumen

3. **Build con modelo incluido**
   - Incluir modelo en imagen Docker (muy pesado)

### Paso 5: Deploy
1. Click en Deploy
2. Esperar build (puede tardar por el modelo)
3. Verificar logs
4. Anotar URL: `https://tu-servicio-1.leapcell.dev`

### Paso 6: Probar
```bash
curl https://tu-servicio-1.leapcell.dev/health
# {"status": "ok"}

curl -X POST https://tu-servicio-1.leapcell.dev/chat \
  -H "Content-Type: application/json" \
  -d '{
    "system": "Responde brevemente",
    "context": {"pregunta": "Hola"},
    "max_tokens": 50
  }'
```

## ‚ö†Ô∏è Consideraciones

### Serverless
- El modelo se carga en cada cold start
- Primera request ser√° lenta (5-30 segundos)
- Requests subsecuentes ser√°n m√°s r√°pidas si el contenedor est√° caliente

### Recursos
- **RAM**: M√≠nimo 2 GB para Qwen 3B
- **CPU**: 1 core m√≠nimo (m√°s es mejor)
- **Storage**: 4-5 GB para el modelo

### Optimizaci√≥n
- Usar modelo cuantizado (Q4_K_M es buena opci√≥n)
- Reducir N_CTX si hay problemas de memoria
- Considerar keep-alive para evitar cold starts

## üîó Integraci√≥n con Otros Servicios
Este servicio debe ser llamado por el Servicio 2 (Backend).

URL del Servicio 1 debe configurarse en Servicio 2:
```bash
MODEL_API_URL=https://tu-servicio-1.leapcell.dev
```

## üìä Monitoreo
- Ver logs en panel de Leapcell
- Endpoint `/health` para health checks
- Monitorear tiempo de respuesta (primera request lenta es normal)

## üêõ Troubleshooting

### Error: "No se encontr√≥ el modelo"
- Verificar `LOCAL_MODEL_PATH`
- Asegurar que el modelo existe en el volumen

### Error: "Out of memory"
- Reducir `N_CTX`
- Usar modelo m√°s peque√±o (1.5B en vez de 3B)
- Aumentar RAM en plan de Leapcell

### Cold start muy lento
- Normal en serverless
- Considerar keep-alive o usar servicio persistente
- Optimizar: pre-cargar modelo en memoria compartida (avanzado)

## üìö Referencias
- [llama-cpp-python docs](https://github.com/abetlen/llama-cpp-python)
- [Modelos GGUF](https://huggingface.co/models?library=gguf)
- [Leapcell docs](https://docs.leapcell.io)
