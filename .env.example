# Servicio 1: Model API (LLM)
# Variables de entorno para despliegue

# Ruta al modelo GGUF (ajustar según tu despliegue)
LOCAL_MODEL_PATH=/app/models/qwen2.5-0.5b-instruct-q4_k_m.gguf

# Tamaño del contexto (tokens)
# Reducir si hay problemas de memoria
N_CTX=1024

# Número de threads de CPU
# Aumentar si tienes más CPUs
N_THREADS=1

# Puerto del servicio
PORT=8001
